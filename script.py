# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Iq4TtOMFxLx0ZAIcCdiwO5R_3QMMP4JK
"""

import pandas as pd
import os

# Read the CSV directly
df = pd.read_csv('/content/Sample - Superstore.csv', encoding='ISO-8859-1')
df = pd.read_csv('/content/Sample - Superstore.csv', encoding='latin1')


# Convert Order Date to datetime
df['Order Date'] = pd.to_datetime(df['Order Date'])

# Create output directory if not exists
output_dir = '/content/daily_orders'
os.makedirs(output_dir, exist_ok=True)

# Group by Order Date and save each group to a CSV file
for date, group in df.groupby(df['Order Date'].dt.date):
    date_str = date.strftime('%Y-%m-%d')  # format: YYYY-MM-DD
    filename = f"{output_dir}/orders_{date_str}.csv"
    group.to_csv(filename, index=False)

print("Date-wise CSV files created successfully.")

import shutil

shutil.make_archive('/content/daily_orders_zip', 'zip', '/content/daily_orders')

from google.colab import files

files.download('/content/daily_orders_zip.zip')

!pip install boto3

import boto3
import os

# Replace with your actual keys and region
aws_access_key = ''
aws_secret_key = ''
aws_region = 'ap-southeast-2'  # or your preferred region

bucket_name = 'luffybucketonepiece'
local_folder = '/content/daily_orders'

# S3 client with credentials
s3 = boto3.client(
    's3',
    aws_access_key_id=aws_access_key,
    aws_secret_access_key=aws_secret_key,
    region_name=aws_region
)

for filename in os.listdir(local_folder):
    if filename.endswith('.csv'):
        date_str = filename.replace('orders_', '').replace('.csv', '')
        s3_key = f"orders/snapshot_day={date_str}/{filename}"
        local_file_path = os.path.join(local_folder, filename)
        s3.upload_file(local_file_path, bucket_name, s3_key)
        print(f"✅ Uploaded: s3://{bucket_name}/{s3_key}")

import time

import boto3
import os
import time

# --- AWS S3 CONFIGURATION ---
aws_access_key = ''
aws_secret_key = ''
region = 'ap-southeast-2'
bucket_name = 'luffybucketonepiece'
local_folder = '/content/daily_orders'  # Adjust if needed

# Create S3 client
s3 = boto3.client(
    's3',
    aws_access_key_id=aws_access_key,
    aws_secret_access_key=aws_secret_key,
    region_name=region
)

# Sort filenames to maintain chronological order
file_list = sorted(f for f in os.listdir(local_folder) if f.endswith('.csv'))

# Upload files one by one every 1 hour
for filename in file_list:
    date_str = filename.replace('orders_', '').replace('.csv', '')
    s3_key = f"orders/snapshot_day={date_str}/{filename}"
    local_file_path = os.path.join(local_folder, filename)

    try:
        s3.upload_file(local_file_path, bucket_name, s3_key)
        print(f"✅ Uploaded: s3://{bucket_name}/{s3_key}")
    except Exception as e:
        print(f"❌ Failed to upload {filename}: {str(e)}")

    # Wait for 1 hour before next upload
    print("⏳ Waiting 1 hour before next file...")
    time.sleep(3600)  # 1 hour = 3600 seconds